{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c152ed03",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 262\u001b[39m\n\u001b[32m    260\u001b[39m \u001b[38;5;66;03m# executa e mostra resultado\u001b[39;00m\n\u001b[32m    261\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m     df = \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscrape_best\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    263\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== Resultado final ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    264\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mShape:\u001b[39m\u001b[33m\"\u001b[39m, df.shape)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/asyncio/runners.py:190\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(main, debug, loop_factory)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[32m    162\u001b[39m \n\u001b[32m    163\u001b[39m \u001b[33;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    186\u001b[39m \u001b[33;03m    asyncio.run(main())\u001b[39;00m\n\u001b[32m    187\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m events._get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    189\u001b[39m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    191\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug=debug, loop_factory=loop_factory) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[32m    194\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m runner.run(main)\n",
      "\u001b[31mRuntimeError\u001b[39m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Scraper robusto para https://www.fundsexplorer.com.br/ranking\n",
    "- Intercepta respostas de rede (XHR/Fetch) via Playwright e tenta extrair JSON com os dados.\n",
    "- Fallback: extrai a tabela do DOM após rolagem e espera.\n",
    "- Normaliza nomes para ASCII, renomeia 'papel'/'fundos' -> 'ticker', tenta converter números.\n",
    "- NÃO salva em Excel; mostra o DataFrame no final.\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError\n",
    "import pandas as pd\n",
    "import unicodedata, re, math, time\n",
    "\n",
    "URL = \"https://www.fundsexplorer.com.br/ranking\"\n",
    "NAV_TIMEOUT_MS = 20000\n",
    "\n",
    "# utilitários\n",
    "def ascii_colname(s: str) -> str:\n",
    "    if s is None:\n",
    "        return s\n",
    "    s = str(s).strip()\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    s = s.encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "    s = re.sub(r\"[^\\w\\s\\-\\%\\/\\(\\)\\.,:$]\", \"\", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s.upper()  # deixo em maiúsculas para visualização consistente\n",
    "\n",
    "def try_parse_number_like(s):\n",
    "    if s is None:\n",
    "        return s\n",
    "    if isinstance(s, (int, float)):\n",
    "        return float(s)\n",
    "    s = str(s).strip()\n",
    "    if s == \"\":\n",
    "        return s\n",
    "    s2 = s.replace(\"R$\", \"\").replace(\"%\", \"\").replace(\".\", \"\").replace(\"\\xa0\", \"\").strip()\n",
    "    # tentar detectar vírgula decimal\n",
    "    if \",\" in s2 and s2.count(\",\") == 1:\n",
    "        s2 = s2.replace(\",\", \".\")\n",
    "    # remover espaços e sinais extras\n",
    "    s2 = re.sub(r\"[^\\d\\-\\.\\+eE]\", \"\", s2)\n",
    "    if s2 in (\"\", \".\", \"-\", \"+\"):\n",
    "        return s\n",
    "    try:\n",
    "        v = float(s2)\n",
    "        return v\n",
    "    except Exception:\n",
    "        return s\n",
    "\n",
    "# detecta se um objeto é plausível como \"lista de registros de tabela\"\n",
    "def is_plausible_table_list(obj):\n",
    "    if not isinstance(obj, list):\n",
    "        return False\n",
    "    if len(obj) == 0:\n",
    "        return False\n",
    "    # verificar se elementos são dicts com múltiplas keys\n",
    "    sample = obj[0]\n",
    "    return isinstance(sample, dict) and len(sample.keys()) >= 2\n",
    "\n",
    "async def fetch_page_and_capture_json(url=URL, headless=True, timeout_ms=NAV_TIMEOUT_MS):\n",
    "    candidates = []  # lista de (url, parsed_json)\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=headless)\n",
    "        page = await browser.new_page()\n",
    "        # coletar respostas\n",
    "        responses = []\n",
    "\n",
    "        def on_response(response):\n",
    "            # coletar apenas respostas com content-type json ou que terminem com .json\n",
    "            try:\n",
    "                rurl = response.url\n",
    "                # armazenar response object; we'll try to json() them later\n",
    "                responses.append(response)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        page.on(\"response\", on_response)\n",
    "\n",
    "        try:\n",
    "            await page.goto(url, timeout=timeout_ms)\n",
    "        except PlaywrightTimeoutError:\n",
    "            await page.goto(url)\n",
    "\n",
    "        # esperar um pouco para XHRs carregarem\n",
    "        await page.wait_for_timeout(2000)\n",
    "        # forçar rolagem para disparar carregamento lazy/virtualized\n",
    "        await page.evaluate(\"\"\"() => { window.scrollTo(0, document.body.scrollHeight); }\"\"\")\n",
    "        await page.wait_for_timeout(1500)\n",
    "        await page.evaluate(\"\"\"() => { window.scrollTo(0, 0); }\"\"\")\n",
    "        await page.wait_for_timeout(500)\n",
    "\n",
    "        # tentar processar as respostas capturadas - pegar as que contem json\n",
    "        for resp in responses:\n",
    "            try:\n",
    "                # filtrar por status 200\n",
    "                status = resp.status\n",
    "                if status != 200:\n",
    "                    continue\n",
    "                # tentar obter header content-type\n",
    "                try:\n",
    "                    headers = await resp.all_headers()\n",
    "                    ct = headers.get(\"content-type\",\"\")\n",
    "                except Exception:\n",
    "                    ct = \"\"\n",
    "                # Preferir JSONs\n",
    "                if \"application/json\" in ct or resp.url.endswith(\".json\") or \"api\" in resp.url.lower() or \"ranking\" in resp.url.lower() or \"funds\" in resp.url.lower():\n",
    "                    # tentar json\n",
    "                    try:\n",
    "                        j = await resp.json()\n",
    "                        candidates.append((resp.url, j))\n",
    "                    except Exception:\n",
    "                        # some responses may not be valid json; skip\n",
    "                        continue\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        await browser.close()\n",
    "    return candidates\n",
    "\n",
    "# fallback DOM extractor: pega tabela visível lendo innerText das células\n",
    "# retorna DataFrame\n",
    "def extract_table_from_grid_js_result(grid):\n",
    "    # grid = list of rows (lists)\n",
    "    if not grid:\n",
    "        return pd.DataFrame()\n",
    "    ncols = max(len(r) for r in grid)\n",
    "    # determinar count header rows by heuristic\n",
    "    def alpha_numeric_counts(row):\n",
    "        letters = sum(1 for c in row if isinstance(c, str) and re.search(r\"[A-Za-zÀ-ÿ]\", c))\n",
    "        digits = sum(1 for c in row if isinstance(c, str) and re.search(r\"[0-9]\", c))\n",
    "        return letters, digits\n",
    "\n",
    "    header_rows = 0\n",
    "    for row in grid:\n",
    "        a,n = alpha_numeric_counts(row)\n",
    "        if a >= n:\n",
    "            header_rows += 1\n",
    "        else:\n",
    "            break\n",
    "    if header_rows == 0:\n",
    "        header_rows = 1\n",
    "\n",
    "    headers = grid[:header_rows]\n",
    "    data = grid[header_rows:]\n",
    "\n",
    "    colnames = []\n",
    "    for j in range(ncols):\n",
    "        parts = [h[j].strip() for h in headers if j < len(h) and isinstance(h[j], str) and h[j].strip() != \"\"]\n",
    "        colnames.append(\" - \".join(parts) if parts else f\"col_{j}\")\n",
    "\n",
    "    normalized_rows = []\n",
    "    for row in data:\n",
    "        new = [ (row[i] if i < len(row) else \"\") for i in range(ncols) ]\n",
    "        normalized_rows.append(new)\n",
    "\n",
    "    df = pd.DataFrame(normalized_rows, columns=colnames)\n",
    "    return df\n",
    "\n",
    "async def dom_extract_with_playwright(url=URL, headless=True, timeout_ms=NAV_TIMEOUT_MS):\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=headless)\n",
    "        page = await browser.new_page()\n",
    "        try:\n",
    "            await page.goto(url, timeout=timeout_ms)\n",
    "        except PlaywrightTimeoutError:\n",
    "            await page.goto(url)\n",
    "        # aguardar tabela ou algumas linhas\n",
    "        try:\n",
    "            await page.wait_for_selector(\"table tbody tr, table tr\", timeout=timeout_ms)\n",
    "        except PlaywrightTimeoutError:\n",
    "            pass\n",
    "        # scroll incremental para forçar renderização de linhas\n",
    "        for y in range(0, 3000, 800):\n",
    "            await page.evaluate(f\"() => window.scrollTo(0, {y});\")\n",
    "            await page.wait_for_timeout(300)\n",
    "        # executar js que constrói grid (respeita colspan/rowspan)\n",
    "        js_extract = \"\"\"\n",
    "        () => {\n",
    "            function tableToGrid(table) {\n",
    "                const rows = Array.from(table.querySelectorAll('tr'));\n",
    "                const grid = [];\n",
    "                const occupied = {};\n",
    "                let maxCols = 0;\n",
    "                for (let r = 0; r < rows.length; r++) {\n",
    "                    if (!grid[r]) grid[r] = [];\n",
    "                    const tr = rows[r];\n",
    "                    const cells = Array.from(tr.querySelectorAll('th,td'));\n",
    "                    let c = 0;\n",
    "                    for (const cell of cells) {\n",
    "                        while (occupied[`${r},${c}`]) c++;\n",
    "                        const rowspan = cell.rowSpan || 1;\n",
    "                        const colspan = cell.colSpan || 1;\n",
    "                        const text = (cell.innerText || \"\").trim();\n",
    "                        while (grid[r].length < c) grid[r].push('');\n",
    "                        grid[r][c] = text;\n",
    "                        for (let rr = r; rr < r + rowspan; rr++) {\n",
    "                            for (let cc = c; cc < c + colspan; cc++) {\n",
    "                                occupied[`${rr},${cc}`] = true;\n",
    "                                if (!grid[rr]) grid[rr] = [];\n",
    "                                while (grid[rr].length <= cc) grid[rr].push('');\n",
    "                                if (!(rr === r && cc === c)) grid[rr][cc] = grid[rr][cc] || '';\n",
    "                            }\n",
    "                        }\n",
    "                        c += colspan;\n",
    "                        if (c > maxCols) maxCols = c;\n",
    "                    }\n",
    "                    while (grid[r].length < maxCols) grid[r].push('');\n",
    "                }\n",
    "                const finalMax = Math.max(...grid.map(r => r.length));\n",
    "                for (let i=0;i<grid.length;i++){\n",
    "                    while (grid[i].length < finalMax) grid[i].push('');\n",
    "                }\n",
    "                return grid;\n",
    "            }\n",
    "            const table = document.querySelector('table');\n",
    "            if (!table) return {error:'no_table'};\n",
    "            return {error:null, grid: tableToGrid(table)};\n",
    "        }\n",
    "        \"\"\"\n",
    "        result = await page.evaluate(js_extract)\n",
    "        await browser.close()\n",
    "    if result is None or result.get(\"error\"):\n",
    "        return pd.DataFrame()\n",
    "    return extract_table_from_grid_js_result(result[\"grid\"])\n",
    "\n",
    "# função principal que combina estratégias\n",
    "async def scrape_best():\n",
    "    # 1) tentar capturar respostas JSON durante o carregamento\n",
    "    candidates = await fetch_page_and_capture_json(headless=True)\n",
    "    # tentar achar candidato plausível\n",
    "    for url, obj in candidates:\n",
    "        # Se próprio obj é lista plausível\n",
    "        if is_plausible_table_list(obj):\n",
    "            df = pd.DataFrame(obj)\n",
    "            print(\"Encontrado JSON direto em:\", url)\n",
    "            return df\n",
    "        # se dict com 'data'/'items'/'results'\n",
    "        if isinstance(obj, dict):\n",
    "            for k in (\"data\",\"items\",\"results\",\"rows\",\"funds\"):\n",
    "                if k in obj and is_plausible_table_list(obj[k]):\n",
    "                    print(f\"Encontrado JSON em {url} -> chave '{k}'\")\n",
    "                    df = pd.DataFrame(obj[k])\n",
    "                    return df\n",
    "    # 2) fallback DOM extraction\n",
    "    print(\"Nenhum JSON válido detectado nas respostas; executando extração via DOM (Playwright).\")\n",
    "    df_dom = await dom_extract_with_playwright(headless=True)\n",
    "    if df_dom.empty:\n",
    "        print(\"Extração DOM retornou DataFrame vazio.\")\n",
    "        return df_dom\n",
    "    # normalizar colunas e converter\n",
    "    df_dom.columns = [ascii_colname(c) for c in df_dom.columns]\n",
    "    # rename common col names to ticker\n",
    "    df_dom = df_dom.rename(columns={c:\"TICKER\" for c in df_dom.columns if c.lower() in [\"fundos\",\"papel\"]})\n",
    "    # tentar converter valores\n",
    "    for col in df_dom.columns:\n",
    "        df_dom[col] = df_dom[col].apply(try_parse_number_like)\n",
    "    return df_dom\n",
    "\n",
    "# executa e mostra resultado\n",
    "if __name__ == \"__main__\":\n",
    "    df = asyncio.run(scrape_best())\n",
    "    print(\"\\n=== Resultado final ===\")\n",
    "    print(\"Shape:\", df.shape)\n",
    "    # mostrar cabeçalho (10 linhas) no console\n",
    "    if df.empty:\n",
    "        print(\"DataFrame vazio — nenhuma estratégia encontrou os dados.\")\n",
    "        print(\"Se possível, abra seu navegador, abra DevTools > Network > XHR e recarregue a página; copie a requisição que retorna os dados da tabela (URL) e cole aqui que eu adapto um requests direto para esse endpoint.\")\n",
    "    else:\n",
    "        # normalizar nomes finais (upper + ASCII) e imprimir\n",
    "        df.columns = [ascii_colname(c) for c in df.columns]\n",
    "        # renomear novamente 'papel'-> ticker se necessário\n",
    "        for c in list(df.columns):\n",
    "            if c.lower() in (\"papel\",\"fundos\"):\n",
    "                df = df.rename(columns={c:\"TICKER\"})\n",
    "        # garantir ticker first column if exists\n",
    "        cols = list(df.columns)\n",
    "        if \"TICKER\" in cols:\n",
    "            cols = [\"TICKER\"] + [c for c in cols if c!=\"TICKER\"]\n",
    "            df = df[cols]\n",
    "        # tentar converter colunas numéricas (já feito parcialmente)\n",
    "        print(df.head(10).to_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
